# ğŸ›ï¸ Sales Pipeline - Databricks Edition
Ce projet implÃ©mente un pipeline de donnÃ©es ELT (Extract, Load, Transform) suivant l'architecture "Medallion" (Bronze, Silver, Gold). Il a pour but de traiter les donnÃ©es de ventes internationales (Paris, New York, Tokyo), de les normaliser et de calculer des KPIs business.
Ce code est conÃ§u pour Ãªtre modulaire, testable et dÃ©ployable sur Databricks ou tout environnement Spark.
## ğŸ¯ But du projet
L'objectif est de transformer des fichiers bruts (CSV) dÃ©posÃ©s dans un volume en tableaux de bord analytiques exploitables.
**Bronze (Ingestion) :** Chargement des donnÃ©es brutes (CSV) vers des tables Delta, avec archivage des fichiers sources.
**Silver (Cleaning) :** Nettoyage, typage, dÃ©doublonnage, traduction des produits et enrichissement (Pays, Devise).
**Gold (Aggregation) :** Conversion des devises en Euro et calcul des KPIs (CA par mois, Top Produits, etc.).

## ğŸ› ï¸ Installation et PrÃ©requis
### 1. Environnement Local
Pour dÃ©velopper ou tester en local, vous avez besoin de Python (>=3.9) et de Java (pour Spark).
Installez les dÃ©pendances et l'environnement de dÃ©veloppement :

`pip install -e .[dev]`
### 2. Sur Databricks
Le projet est conÃ§u pour Ãªtre exÃ©cutÃ© via Databricks Repos (Git Folders) ou packagÃ© sous forme de fichier .whl.
L'environnement Databricks fournit nativement Spark.
## ğŸš€ Comment exÃ©cuter le pipeline
### En local (pour le dÃ©veloppement)
Le point d'entrÃ©e unique est le fichier main.py. Il orchestre les trois Ã©tapes (Bronze -> Silver -> Gold).

`python main.py`
### Sur Databricks (Production)
CrÃ©er un Job Databricks.
Configurer la tÃ¢che pour utiliser le script main.py (via Git Source).
Assurez-vous que le cluster a accÃ¨s au volume de donnÃ©es dÃ©fini dans config/config.yaml.
## ğŸ§ª Lancer les tests
Les tests unitaires garantissent que la logique de transformation (Silver/Gold) ne rÃ©gresse pas. Ils utilisent une session Spark locale.
`pytest tests/`
